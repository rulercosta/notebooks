{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rulercosta/notebooks/blob/main/Essential_MML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvSApRDbQLe0"
      },
      "source": [
        "# **1. LINEAR ALGEBRA**\n",
        "\n",
        "Linear algebra is a branch of mathematics that is widely used throughout science and engineering. Linear algebra is a form of continuous mathematics. Linear algebra is the branch of mathematics concerning linear equations such as: linear maps, and their representations in vector spaces and through matrices. Linear algebra is central to almost all areas of mathematics.\n",
        "\n",
        "---\n",
        "## **1.0. Motivation for learning Linear Algebra**\n",
        "\n",
        "There are several prerequisites for better understanding of Machine Learning and (especially) Deep Learning algorithms. In order to grasp the nitty gritties of higher ML concepts, one needs to have a strong base of higher mathematics.\n",
        "\n",
        "Linear Algebra is the mathematical foundation that solves the problem of representing data as well as computations in machine learning models.\n",
        "\n",
        "Hence, the concepts of linear algebra are crucial for understanding the theory behind machine learning, especially for deep learning.\n",
        "\n",
        "---\n",
        "\n",
        "## **1.1. Mathematical Objects**\n",
        "\n",
        "Mathematical objects are what we talk and write about when we do math. Numbers, functions, triangles, matrices, groups and more complicated things such as vector spaces and infinite series are all examples of mathematical objects. Math objects are abstract objects. They are not physical objects, but we think about them and talk about them as if they actually existed. Math objects have certain properties that other kinds of abstract objects may not have. In particular, unlike other kinds of abstract objects, math objects are inert:\n",
        "*   Math objects don't move or change over time.\n",
        "*   Math objects don't interact with other objects or with the real world.\n",
        "\n",
        "In particular, we are concerned with the study of following mathematical objects in Linear Algebra amongst others:\n",
        "\n",
        "\n",
        "1.   Scalars\n",
        "2.   Vectors\n",
        "3.   Matrices\n",
        "4.   Tensors\n",
        "\n",
        "Furthermore, it should be noted that higher mathematics is nothing but layers upon layers of abstraction. Intuitively, it would be right to consider **Sets** as the foundation of Linear Algebra-\n",
        "*   a line is a set of points\n",
        "*   a plane is a set of lines\n",
        "\n",
        "Similarly,\n",
        "\n",
        "*   a vector is a set of points\n",
        "*   a matrix is a set of vectors\n",
        "\n",
        "Therefore, an understanding of Set Theory is essential to grasp Linear Algebra with ease.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1.1. Scalars\n",
        "\n",
        " A scalar is just a single number, in contrast to most of the otherobjects studied in linear algebra, which are usually arrays of multiple numbers. When we introduce them, we specify what kind of number they are. For example, we might say\n",
        "\n",
        "\\begin{align}\n",
        "\\text {Let}\\ \\ s\\ \\in\\ \\mathbb{R}\\ \\ \\text {be the slope of the line}.\n",
        "\\end{align}\n",
        "\n",
        "while deﬁning a real-valued scalar, or\n",
        "\n",
        "\\begin{align}\n",
        "\\text {Let}\\ \\ n\\ \\in\\ \\mathbb{N}\\ \\ \\text {be the number of units}.\n",
        "\\end{align}\n",
        "\n",
        "while deﬁning a natural number scalar.\n",
        "\n",
        "### 1.1.2. Vectors\n",
        "\n",
        "A vector is an array of numbers. The numbers are arranged inorder. We can identify each individual number by its index in that ordering.\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{x} := \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix}, x_{i,j} \\in \\mathbb{R}\n",
        "\\end{align}\n",
        "\n",
        "Frst element of $\\mathbf{x}$ is $\\mathbf{x}_{1}$, the second element is $\\mathbf{x}_{2}$, and so on.\n",
        "\n",
        "If $\\ \\mathbf{x}_{i} \\ \\in \\ \\mathbb{R} \\ \\ \\forall \\ \\ i \\ \\in \\ \\{ 1,2,3,...,n\\}$, then the vector $\\mathbf{x} \\ $, having $\\ n \\ $ dimensions, lies in the set formed by taking the Cartesian product of $\\ \\mathbb{R} \\ \\ n$ times, denoted as $\\ \\mathbf{x} \\ \\in \\ \\mathbb{R}^{n}.$\n",
        "\n",
        "One can think of vectors as identifying points in space, with each element giving the coordinate along a diﬀerent axis. Sometimes we need to index a set of elements of a vector. In this case, we deﬁne a set containing the indices and write the set as a subscript.\n",
        "\n",
        "For example, to access $\\ \\mathbf{x}_1$, $\\ \\mathbf{x}_3\\ $ and $\\ \\mathbf{x}_6$, we deﬁne the set $\\ S = \\{1,3,6\\}\\ $ and write $\\ \\mathbf{x}_S.$ We use the $\\ −\\ $ sign to index the complement of a set. For example, $\\ \\mathbf{x}_{-1}\\ $ is the vector containing all elements of $\\ \\mathbf{x}\\ $ except for $\\ \\mathbf{x}_1.$ Similarly, $\\ \\mathbf{x}_S\\ $ is the vector containing all elements of  $\\ \\mathbf{x}\\ $ except for $\\ \\mathbf{x}_1$, $\\ \\mathbf{x}_3\\ $ and $\\ \\mathbf{x}_6.$\n",
        "\n",
        "\\begin{align}\n",
        "f\\left(x\\right) = y, \\\\\n",
        "\\text{for $\\ x \\in \\mathbf{x}$ and $\\ y \\in \\mathbf{y}$.}\n",
        "\\end{align}\n",
        "\n",
        "The ultimate goal of Machine Learning is learning functions from data, i.e., transformations or mappings from the domain onto the range of a function.\n",
        "\n",
        "The domain $\\ \\mathbf{x} \\ $ is usually a vector of variables or features mapping onto a vector of target values.\n",
        "\n",
        "### 1.1.3. Matrices\n",
        "\n",
        " A matrix is a 2-D array of numbers, so each element is identiﬁed by two indices instead of just one.\n",
        "\n",
        " \\begin{align}\n",
        " \\mathbf{A} := \\begin{bmatrix} {A}_{1,1} & {A}_{1,2} & \\ldots & {A}_{1,n}\\\\ {A}_{2,1} & {A}_{2,2} & \\ldots & {A}_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ {A}_{m,1} & {A}_{m,2} & \\ldots & {A}_{m,n}\\end{bmatrix}, A_{i,j} \\in \\mathbb{R}\n",
        " \\end{align}\n",
        "\n",
        "If a real-valued matrix $\\ \\mathbf{A}\\ $ has a height of $\\ m\\ $ (rows) and a width of $\\ n\\ $ (columns), then we say that $\\ \\mathbf{A}\\ \\in \\ \\mathbb{R}^{mn}.$\n",
        "\n",
        "$i^{th}\\ $ row of $\\ \\mathbf{A}\\ $ is denoted by $\\ \\mathbf{A}_{i,:}\\ $ and $j^{th}\\ $ row of $\\ \\mathbf{A}\\ $ is denoted by $\\ \\mathbf{A}_{:,j}.\\ $\n",
        "\n",
        "Suppose by applying some function $\\ f\\ $ to $\\ \\mathbf{A}\\ $ the resultant matrix is $\\ \\mathbf{B}\\ $ then the $\\ (i,j)^{th}\\ $ element of $\\ \\mathbf{B}\\ $ is given by $\\ f(\\mathbf{A})_{i,j}.\\ $\n",
        "\n",
        "\n",
        "\n",
        "### 1.1.4. Tensors\n",
        "\n",
        "A tensor is an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. Suppose a tensor named $\\ \\mathsf{A}\\ $ has three dimensions then the element of $\\ \\mathsf{A}\\ $ at coordinates $\\ (i, j, k)\\ $ is denoted as $\\ \\mathsf{A}_{i,j,k}.$\n",
        "\n",
        "### A note on the transpose of matrices:\n",
        "\n",
        "The transpose of a matrix is the mirror image of the matrix across it's principle diagonal line. The transpose of a matrix $\\ \\mathbf{A}\\ $ is denoted by $\\ \\mathbf{A}^{\\mathsf{T}}.$\n",
        "\n",
        " \\begin{align}\n",
        " \\mathbf{A} = \\begin{bmatrix} {A}_{1,1} & {A}_{1,2} & \\ldots & {A}_{1,n}\\\\ {A}_{2,1} & {A}_{2,2} & \\ldots & {A}_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ {A}_{m,1} & {A}_{m,2} & \\ldots & {A}_{m,n}\\end{bmatrix}.\\\\\n",
        " \\end{align}\n",
        "\n",
        " \\begin{align}\\\\\n",
        " \\mathbf{A}^{\\mathsf{T}} = \\begin{bmatrix} {A}_{1,1} & {A}_{2,1} & \\ldots & {A}_{m,1}\\\\ {A}_{1,2} & {A}_{2,2} & \\ldots & {A}_{m,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ {A}_{1,n} & {A}_{2,n} & \\ldots & {A}_{m,n}\\end{bmatrix}.\n",
        " \\end{align}\n",
        "\n",
        "Vectors can be thought of as matrices that contain only one column. Thus, the transpose of a vector is therefore a matrix with only one row. This can be denoted as $\\ \\mathbf{x} = {\\begin{pmatrix}x_{1},x_{2},x_{3}\\end{pmatrix}}^{\\mathsf{T}}.$\n",
        "\n",
        "A scalar can be thought of as a matrix with only a single entry. From this, we can see that a scalar is its own transpose: $\\ a = a^{\\mathsf{T}}.$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCGM9f_joRji"
      },
      "source": [
        "##**1.2. Properties of matrix operations**\n",
        "\n",
        "The operations are as follows:\n",
        "\n",
        "###1.2.1. Addition:\n",
        "\n",
        "If $\\mathbf{A}$ and $\\mathbf{B}$ are matrices of the same size $m\\times n$, then $\\mathbf{A} + \\mathbf{B}$,\n",
        "their sum, is a matrix of size $m\\times n.$\n",
        "\n",
        "###1.2.2. Multiplication by scalars:\n",
        "\n",
        "If $\\mathbf{A}$ is a matrix of size $m\\times n$ and $\\alpha$ is a scalar,\n",
        "then $\\alpha\\mathbf{A}$ is a matrix of size $m\\times n.$\n",
        "\n",
        "###1.2.3. Matrix multiplication:\n",
        "\n",
        "If $\\mathbf{A}$ is a matrix of size $m\\times n$ and $\\mathbf{B}$ is a matrix of size $n\\times p$, then the product $\\mathbf{AB}$ is a matrix of size $m\\times p.$\n",
        "\n",
        "###1.2.4. Vectors:\n",
        "\n",
        "A vector of length $n$ can be treated as a matrix of size $\\ n \\times\n",
        "1,\\ $ and the operations of vector addition, multiplication by scalars, and\n",
        "multiplying a matrix by a vector agree with the corresponding matrix\n",
        "operations.\n",
        "\n",
        "###1.2.5. Transpose:\n",
        "\n",
        "If $\\mathbf{A}$ is a matrix of size $m\\times n$, then its transpose $\\mathbf{A}^{\\mathsf{T}}$\n",
        "is a matrix\n",
        "of size $n\\times m.$\n",
        "\n",
        "###1.2.6. Identity matrix:\n",
        "\n",
        "An identity matrix is a matrix that does not change any vector when wemultiply that vector by that matrix. We denote the identity matrix that preserves $n$-dimensional vectors as $\\mathbf{I}_{n}.\\ $ Formally, $\\mathbf{I}_{n}\\in\\mathbb{R}_{n\\times n},\\ $ and\n",
        "\n",
        "\\begin{align}\n",
        "\\forall\\ \\mathbf{x}\\in\\mathbb{R}_n,\\ \\mathbf{I}_{n}\\mathbf{x} = \\mathbf{x}.\n",
        "\\end{align}\n",
        "\n",
        "$\\mathbf{I}_{n}\\ $ is the $n\\times n$ identity matrix; its principle diagonal elements are\n",
        "equal to $1$ and its offdiagonal elements are equal to $0.$\n",
        "\n",
        "###1.2.7. Zero matrix:\n",
        "\n",
        "It is denoted by $0$ the matrix of all zeroes (of relevant size).\n",
        "\n",
        "###1.2.8. Inverse:\n",
        "\n",
        "If $\\mathbf{A}$ is a square matrix, then its inverse $\\mathbf{A}^{\\mathsf{-1}}$\n",
        "is a matrix of the\n",
        "same size. The matrices that have nonzero determinant have inverses, and are called invertible.\n",
        "\n",
        "For square matrices,\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{A}^{-1}\\mathbf{A}=\\mathsf{I}_{n}\n",
        "\\end{align}\n",
        "\n",
        "*In many cases, we can treat addition and multiplication of matrices as addition and multiplication of numbers. However, there are some differences between operations with matrices and operations with numbers:*\n",
        "\n",
        "• Properties such as associative, distributive and commutative are followed in scalar multiplication and matrix addition.\n",
        "\n",
        "• Matrix multiplication does not commute.\n",
        "\n",
        "• In general, $\\mathbf{AB} \\not= \\mathbf{BA}$, even if $\\mathbf{A}$ and $\\mathbf{B}$ are both square matrices. If $\\mathbf{AB} = \\mathbf{BA}$,\n",
        "then we say that $\\mathbf{A}$ and $\\mathbf{B}$ commute.\n",
        "\n",
        "• For a general matrix $\\mathbf{A}$, we cannot say that $\\mathbf{AB} = \\mathbf{AC}$ yields  $\\mathbf{B} = \\mathbf{C}.$\n",
        "(However, if we know that $\\mathbf{A}$ is invertible, then we can multiply both sides\n",
        "of the equation $\\mathbf{AB} = \\mathbf{AC}$ to the left by $\\mathbf{A}^{\\mathsf{-1}}$ and get $\\mathbf{B} = \\mathbf{C}.$)\n",
        "\n",
        "• The equation $\\mathbf{AB} = 0 $ does not necessarily yield $\\mathbf{A}=0\\ \\text{ or }\\ \\mathbf{B}=0.$ For\n",
        "example, take:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{bmatrix},\\\n",
        "\\mathbf{B}=\n",
        "\\begin{bmatrix}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y0zBLnin7pr"
      },
      "source": [
        "###A note on the methods of solving a system of linear equations:\n",
        "\n",
        "Apart from the usual (direct) methods of solving a system of linear equations, which includes, Elimination Method, Substitution Method, and Cross multiplication Method, there are various other methods of solving a system of linear equations:\n",
        "\n",
        "* Matrix Method:\n",
        "  * Crammer's Rule\n",
        "  * Gaussian Elimination\n",
        "  * Gauss-Jordan Method\n",
        "  * Traingularization Method\n",
        "  * Choleskey Method\n",
        "  * Partition Method\n",
        "\n",
        "* Iterative methods:\n",
        "  * Jacobi Iterative Method\n",
        "  * Gauss-Seidel Iterative Method\n",
        "  * SOR Method\n",
        "\n",
        "It might seem overwhelming at first but as other properties of and special types of matrices are introduced, these methods will become easier to grasp. However, it must be noted that when it comes to solving a system of linear equations, in our case, we require a fast and efficient algorithm, so it would be fine not having some if not most of these methods on your fingertips.\n",
        "\n",
        "**Note:** *This section will be expanded in the future.*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlPnW_IakcK6"
      },
      "source": [
        "##**1.3. System of Linear Equations**\n",
        "\n",
        "Using the linear algebra notations and the now defined mathematical objects, we can write a system of linear equations as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{A}\\mathbf{x}=\\mathbf{b}\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\ $ is a known matrix, $\\mathbf{b}\\in\\mathbb{R}^{m}\\ $ is a known vector, and $\\mathbf{x}\\in\\mathbb{R}^{n}\\ $ is a vector of unknown variables.\n",
        "\n",
        "In general, an element of vector $\\mathbf{b}:$\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{b}_{i} = \\mathbf{A}_{i,:}\\ \\mathbf{x}\n",
        "\\end{align}\n",
        "\n",
        "Matrix-vector product notation provides a more compact representation forequations of this form.\n",
        "\n",
        "In order to solve for the unknown vector in the equation,\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{A}\\mathbf{x}=\\mathbf{b}\n",
        "\\end{align}\n",
        "\n",
        "pre-multiply both sides of the equation by the inverse of matrix $\\ \\mathbf{A},$\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x}& =\\mathbf{A}^{-1}\\mathbf{b}\\\\\n",
        "\\mathbf{I}_{n}\\mathbf{x}& =\\mathbf{A}^{-1}\\mathbf{b}\\\\\n",
        "\\mathbf{x}& =\\mathbf{A}^{-1}\\mathbf{b}.\n",
        "\\end{align}\n",
        "\n",
        "When $\\mathbf{A}^{-1}$ exists, several diﬀerent algorithms can ﬁnd it in closed form. In theory, the same inverse matrix can then be used to solve the equation many times for diﬀerent values of $\\mathbf{b}.\\ $ $\\mathbf{A}^{-1}\\ $ is primarily useful as a theoretical tool, however, and should not actually be used in practice for most software applications. Because $\\mathbf{A}^{-1}\\ $ can be represented with only limited precision on a digital computer, algorithms that make use of the value of $\\mathbf{b}$ can usually obtain more accurate estimates of x.\n",
        "\n",
        "For $\\mathbf{A}^{-1}$ to exist, equation $\\ \\mathbf{A}\\mathbf{x}=\\mathbf{b}\\ $ must have *exactly one solution for every value of $\\ \\mathbf{b},$* i.e. unique value of $\\ \\mathbf{x}\\ $ for every value of $\\ \\mathbf{b}.$\n",
        "\n",
        "It is also possible for the system of equations to have no solutions or inﬁnitely many solutions for some values of $\\ \\mathbf{b}.$ It is not possible, however, to have more than one but less than inﬁnitely many solutions for a particular $\\ \\mathbf{b};\\ $ if both $x\\ \\text{and}\\ y$ are solutions, then\n",
        "\n",
        "\\begin{align}\n",
        "z = \\alpha{x} + (1 −\\alpha){y}\n",
        "\\end{align}\n",
        "\n",
        "is also a solution for any real $\\ \\alpha.$\n",
        "\n",
        "To analyze how many solutions the equation has, think of the columns of $\\ \\mathbf{A}\\ $ as specifying diﬀerent directions we can travel in from the origin (the point speciﬁedby the vector of all zeros), then determine how many ways there are of reaching  $\\ \\mathbf{b}.\\ $ In this view, each element of x speciﬁes how far we should travel in each of these directions, with $\\ \\mathbf{x}_{i}\\ $ specifying how far to move in the direction of column i:\n",
        "\\begin{align}\n",
        "\\mathbf{A}\\mathbf{x}=\\sum _{i}\\mathbf{x}_{i}\\mathbf{A}_{:,i}.\n",
        "\\end{align}\n",
        "\n",
        "In general, this kind of operation is called a ***Linear Combination.*** Any n-vector can be represented as a linear combination of the standard unit vectors.\n",
        "For example:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{x} &= \\begin{pmatrix}a\\\\ b\\\\ c\\end{pmatrix} &\\\\ \\\\\n",
        "&= a\\begin{pmatrix}1\\\\ 0\\\\ 0\\end{pmatrix}+b\\begin{pmatrix}0\\\\ 1\\\\ 0\\end{pmatrix}+c\\begin{pmatrix}0\\\\ 0\\\\ 1\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Formally, a linear combination of some set of vectors $\\ \\{\\mathbf{v}^{(1)},\\ldots, \\mathbf{v}^{(n)} \\}\\ $ is given by multiplying each vector $\\ \\mathbf{v} $ by a corresponding scalar coeﬃcient and adding the results:\n",
        "\n",
        "\\begin{align}\n",
        "\\sum _{i}{c}_{i}\\mathbf{v}^{(i)}.\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "This is known as the ***Span.*** The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors. This particular span is known as the **column space,** or the **range,** of $\\ \\mathbf{A}.$\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1IW9pqsSZCcv7TWcNdDL15fRR6dUx130l)\n",
        "\n",
        "\n",
        "**Figure:** *Span of vectors*\n",
        "\n",
        "\n",
        "Solution of the equation $\\ \\mathbf{A}\\mathbf{x}=\\mathbf{b}\\ $ exists if $\\ \\mathbf{b}\\ $ lies in the span of the columns of $\\ \\mathbf{A}.\\ $ And to have a solution for all values of $\\ \\mathbf{b}\\in\\mathbb{R}^{m},\\ $ the column space of $\\ \\mathbf{A}\\ $ must be all of $\\ \\mathbb{R}^{m}\\ $ which implies that $\\ \\mathbf{A}\\ $ must have atleast $\\ m\\ $ columns ($n>m$)\n",
        "Otherwise, the dimensionality of $\\ \\mathbf{A}\\ $ would be reduced (for potential value of $\\ \\mathbf{b}\\ $ there would be no solution) For example, consider a $\\ 3\\times 2\\ $ matrix. The span of vectors results in a $\\ 2D\\ $ plane within $\\ \\mathbb{R}^{3}\\ $ but the target $\\ \\mathbf{b}\\ $ is $\\ 3D.\\ $\n",
        "Modifying the value of $\\ \\mathbf{x}\\ $ at best enables us to trace out a $\\ 2D\\ $ plane within $\\ \\mathbb{R}^{3}.\\ $ The equation has a solution if and only if $\\ \\mathbf{b}\\ $ lies on that plane.\n",
        "\n",
        "Now, $\\ n>m\\ $ is only a necessary condition for every point to have a solution.\n",
        "\n",
        "It is possible for some of the columns to be redundant (mutually linearly dependent; one vector being linear combination of another vector) which implies that addition of any linearly dependent vector to a set of vectors does not contribute any new points to the span of that set of vectors.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1jMjIRpUus6QBjoci4enTx2IaMqxTpmYX)\n",
        "\n",
        "\n",
        "**Figure:** *Set of two or more vectors are said to be linearly dependent if atleast one of the vectors can be obtained as a linear combination of other vectors in the set. This is known as Linear Dependence.*\n",
        "\n",
        "\n",
        "This means that for the column space of the matrix to encompass all of $\\ \\mathbb{R}^{m}\\ $, the matrix must contain at least one set of *exactly* $\\ m\\ $ linearly independent columns. This condition is both necessary and suﬃcient for solutions to all values of $\\ \\mathbf{b}\\in\\mathbb{R}^{m}.$\n",
        "\n",
        "\n",
        "**Note:** *No set of $\\ m\\ $ dimensional vectors can have more than $\\ m\\ $ mutually linearly independent columns, but a matrix with more than $\\ m\\ $ columns may have more than one such set.*\n",
        "\n",
        "\n",
        "We also need to ensure that the equation is over-constrained, thus depending on the nature of linear dependence of the columns of the matrix, the system of equations has either zero (when it is not possible to satisfy all equations simulatneously, in case of mutually linearly independent columns) or infinitely many solutions (when it is possible to represent the general solution as the linear combination of vectors using arbitrary constants, in case of mutually linearly independent columns) Thus, the matrix must have atmost $\\ m\\ $ columns, so that the equation has atmost one solution for each value of $\\ b.$\n",
        "\n",
        "It is now evident that in the coefficient matrix of the equation $\\ \\mathbf{A}\\mathbf{x}=\\mathbf{b}\\ $ all the columns of the matrix must be linearly independent and that the number of columns must be equal to number of rows of the matrix i.e. $\\ n=m.\\ $ This means that the matrix $\\ \\mathbf{A}\\ $ is a *square matrix.*\n",
        "\n",
        "A square matrix with linearly dependent columns is known as *singular* (zero determinant) A square matrix has linearly independent columns iff the matrix is *nonsingular.*\n",
        "\n",
        "Thus, it is only possible to find inverse of a non-singular square matrix, having linearly independent columns.\n",
        "\n",
        "\n",
        "**Note:** *The Matrix Inversion Method of solving a system of equations cannot be applied in cases where we cannot find the inverse of the coefficient matrix (obviously) Such as in cases where the matrix is not square or it is square but it is also singular (has a zero determinant) In such cases, we shall use methods other than Matrix Inversion method in order to solve for a system of linear equations.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jqlEXdtB81G"
      },
      "source": [
        "# **1.4. Norms**\n",
        "\n",
        "In Machine Learning, the size of vectors is measured using a function called a **norm**. Formally, the $\\ L^{p}\\ $ norm is given by:\n",
        "\n",
        "\\begin{align}\n",
        "\\|\\mathbf{x}\\|_{p} \\  = \\left( \\sum_i \\left| x_i \\right| ^p \\right) ^ {\\frac {1}{p}}\n",
        "\\end{align}\n",
        "\n",
        "for $\\ p \\in \\mathbb{R}, p \\ge 1. \\ $\n",
        "\n",
        "The $\\ L^{p}\\ $ norm is a *mapping function.* It is used to map vectors to non-negative values. It satisfies the following properties:\n",
        "\n",
        "*   $\\ f(x) = 0 \\implies x = 0 \\ $\n",
        "*   $\\ f(x+y) \\le f(x) + f(y)\\ $ (the **triangle inequality**)\n",
        "*   $\\ \\forall \\alpha \\ \\in \\mathbb{R}, f(\\alpha x) = \\left| \\alpha \\right|f(x) \\ $\n",
        "\n",
        "Norm of a vector $\\ \\mathbf{x} \\ $ can be visualized as the distance of the point $\\ \\mathbf{x} \\ $ from the origin in the vector space.\n",
        "\n",
        "For a vector $\\ \\mathbf{x} \\ ,$ the $\\ L^{2} \\ $ norm, also known as **Euclidean norm**.\n",
        "\n",
        "For sake of simplicity, one can imagine a vector space of $2$ dimensions i.e., in an $\\ xy\\ $ plane, vector $\\ \\mathbf{x} \\ $ defined as:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{x} = x \\hat{i} + y \\hat{j}\n",
        "\\end{align}\n",
        "\n",
        "Then the distance of the *point* $\\ \\mathbf{x} \\ $ from the origin, is given by the **Distance formula** as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\ \\|\\mathbf{x}\\| = \\sqrt{x_1^2 + \\dots + x_n^2} \\\\\n",
        "\\end{align}\n",
        "\n",
        "In essence, this calculated distance is the **Euclidean distance** of the vector $\\ \\mathbf{x}.\\ $\n",
        "\n",
        "For a vector $\\ \\mathbf{x} \\ $ of $n$ dimensions,\n",
        "\n",
        "\\begin{align}\n",
        "L^{2} &= \\|\\mathbf{x}\\|_2 = \\sqrt{x_1^2 + \\dots + x_n^2}\n",
        "\\end{align}\n",
        "\n",
        "In machine learning, Euclidean norm is quite frequently used (and as seen from the co-ordinate geometry analogy) it is represented simply by $\\ \\|\\mathbf{x}\\|.\\ $\n",
        "\n",
        "\\begin{align}\n",
        "L^{2} &= \\|\\mathbf{x}\\|_2 = \\|\\mathbf{x}\\| = \\sqrt{x_1^2 + \\dots + x_n^2} \\\\\n",
        "\\left( L^{2} \\right) ^{2} &= \\|\\mathbf{x}\\|_2^2 = \\|\\mathbf{x}\\|^{2} = x_1^2 + \\dots + x_n^2\n",
        "\\end{align}\n",
        "\n",
        "Mathematically and computationally, the squared $\\ L^{2} \\ $ norm is more convenient to use, for example: the derivatives of the squared $\\ L^{2}\\ $ norm with respect to each element of $\\ \\mathbf{x} \\ $ each depend only on the corresponding element of $\\ \\mathbf{x} \\ $ while all of the derivatives of the $\\ L^{2} \\ $ norm depend on the entire vector.\n",
        "\n",
        "For a vector $\\ \\mathbf{x}:$\n",
        "\n",
        "\\begin{align}\n",
        "\\|\\mathbf{x}\\| = \\mathbf{x}^{T}\\mathbf{x}\n",
        "\\end{align}\n",
        "\n",
        "the $\\ L^{2} \\ $ norm can easily be calculated.\n",
        "\n",
        "However, in several Machine Learning applications, where it is necessary to differentiate between zero and non-zero elements, in such cases $\\ L^{1} \\ $ norm is preferred as opposed to the $\\ L^{2} \\ $ norm (which grows very slowly near the origin).\n",
        "\n",
        "\\begin{align}\n",
        "\\|\\mathbf{x}\\|_{1} = \\sum_{i} \\left|x_{i}\\right|.\n",
        "\\end{align}\n",
        "\n",
        "Every time an element of vector $\\ \\mathbf{x} \\ $ is offset from the origin by $\\ ϵ \\ $ the $\\ L^{1} \\ $ norm increases by $\\ ϵ.\\ $ The linear norm is also used as a subsitute for the number of non-zero entries.\n",
        "\n",
        "Two special norms which are used quite commonly in the context of machine learning and deep learning are:\n",
        "\n",
        "*   **Max norm**\n",
        "\n",
        "\\begin{align}\n",
        "\\|\\mathbf{x}\\|_{\\infty} = \\max_{i} \\left|x_{i}\\right|.\n",
        "\\end{align}\n",
        "Max norm of a vector gives the absolute value of the element having the largest value in the vector\n",
        "\n",
        "*   **Frobenius norm**\n",
        "\n",
        "\\begin{align}\n",
        "\\|A\\|_{F} = \\sqrt{\\sum_{i,j}A^{2}_{i,j}},\n",
        "\\end{align}\n",
        "\n",
        "which is analogous to the $\\ L^{2} \\ $ norm of a vector.\n",
        "\n",
        "The dot product of two vectors can be written in terms of norms. Specifically using the $\\ L^{2} \\ $ norm as followed:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{T}\\mathbf{y} = \\|\\mathbf{x}\\|_{2}\\|\\mathbf{y}\\|_{2}\\cos\\theta \\ ,\n",
        "\\text{where $\\theta$ is the angle between $\\mathbf{x}$ and $\\mathbf{y}$}.\n",
        "\\end{align}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}